{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Overview\n",
    "\n",
    "Reinforcement Learning: An Introduction\n",
    "   \n",
    "2nd Edition Completed Draft, by Richard S. Sutton and Andrew G. Barto\n",
    "   \n",
    "http://incompleteideas.net/book/bookdraft2018jan1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Origins of Reinforcement Learning\n",
    "\n",
    "When it comes to analyzing the origins of modern Reinforcement Learning, there are three separate starting points that eventually merge to form what we know today: Optimal Control, Trial and Error through Animal Learning, and, less prevalent, Temporal-Difference Methods. First we will be starting with optimal control.\n",
    "\n",
    "Around the mid-1950s, Richard Bellman and others tackled the problem of \"optimal control\", described as minimizing a metric of a constantly changing enviroment over time. By combining the system's own state and a value function, optimized for a certain return goal, they were able to create a functional equation, one that is now known as the Bellman equation (Maybe have an explanation of that now?).\n",
    "\n",
    "This marks the beginning of what we now know as dynamic programming, the process of solving complex problems by breaking them down into subproblems and building upon each smaller solved one. Bellman also is credited with creating the Markovian decision process (MDPs) while Ronald Howard added on to MDPs by making the policy iteration method for them.\n",
    " \n",
    "The next major part is trial and error through animal learning, a practice, that according to American spychologist R. S. Woodworth, goes as far back as the late 1850s. One of the first few to truly recognize the concept of trial-and-error was Edward Thorndike, an American psychologist that worked extensively on comparitive psychology and the learning process. He initially stated what is now know as \"The Law of Effect\", a law that describes the correlation between reinforcing events and choosing actions. Over time, the theory was adapted to and laid the foundations for many professionals in the field, such as Pavlov and B. F. Skinner. \n",
    "\n",
    "(talk about Turing, advancements in analog RL techniques)\n",
    "In 1948, Alan Turing described a \"pleasure-pain system\"\n",
    "\n",
    "However, due to a lot of confusion in the previous decades due to people using the words reinforcement learning and other types of learning (such as perceptual and supervised) as synonyms, there was a period of silence where development in the field proved slow. Although, there were some exceptions to this trend. The terms \"reinforcement\" and \"reinforcement learning\" were actually used in scientific literature for the first time. This is also the time period where Minksky's paper \"Steps Toward Artificial Intelligence\" that talked about the problem of \"How  do  you  distribute  credit  for  success  among  the  many  decisions  that  may  have  been involved in producing it?\". Many topics in this paper are still relevant today. Some other examples are the system STeLLA by John Andreae and MENACE by Donald Michie.\n",
    "\n",
    "One person in particular who is attributed to reviving the field is Harry Klopf, who recognized that there were characteristics of \"adaptive behavior\" that were being fully ignored. The idea he proposed was the drive to reach a goal in the enviroment, to have a clear desired outcome and undesired end. Eventually, this push evolved into the official distinction between supervised and reinforcement learning.\n",
    "\n",
    "As mentioned previously, this is the third and last part regarding the origins of reinforcement learning: temporal-difference learning. This type of learning can best be described as (). The origins of this concept can be attributed to animal learning psychology, specifically in the idea of secondary reinforcers. A second reinforcer is a stimulus that has been passively associated with with a primary reinforer (example needed) and thus has a similar effect. \n",
    "\n",
    "(Sutton work)\n",
    "\n",
    "In 1989, Chris Watkin's converged the major parts discussed before into developing Q-Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States and Actions\n",
    "\n",
    "![alt text](images/states_actions.png)\n",
    "\n",
    "The first core concept we will cover is the understanding of what states and actions are. Reinforcement learning is a type of machine learning that is agent-oriented; it relies on its enviroment rather than a teacher to achieve its desired goal. This is similar to how humans learn, through the steps of trial and error.\n",
    "\n",
    "Let's take for example a person learning to navigate a maze. A state can compropise of any crossroad they are met with, an action is defined as a choice/direction they choose to go, and the goal (reward) is defined as them reaching the end of the maze.\n",
    "\n",
    "As the person navigates the maze, they will naturally discover that some paths are less optimal than others, while some do not ever reach the end. Ideally, over time, they would be able to navigate the most optimal path every time. And this is what we are trying to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process \n",
    "\n",
    "Building on top of states and actions is the next step, a Markov Decision Process (MDP). A MDP can be simplified to a tuple containing 5 parts:\n",
    "   \n",
    "S - set of states   \n",
    "A - set of actions   \n",
    "P - probability that an action *a* at state *s* at time *t* will get to state *s + 1* at time *t + 1*   \n",
    "R - reward received after moving from state *s* to state *s + 1*   \n",
    "$\\gamma$ - discount factor that can optimize future rewards vs present rewards\n",
    "   \n",
    "Each of these play a role in determining a final \"policy\" $\\pi$; a rule that says given a state *s*, action *a* will be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](images/markov.png)\n",
    "\n",
    "This is the standard relationship between an Agent and the Enviroment in a MDP. An agent is the one who learns and makes decision while the enviroment is everything outside of the agent. These two variables constantly interact and feed each other data, with the enviroment supplying the agent with rewards and the agent triggering the effects of the enviroment.\n",
    "\n",
    "*(Not sure if I want to include this or not, talks about why the distribution of S and R is only dependent on the prior state and action values).*\n",
    "$$p({s',r | s,a}) \\doteq P\\{S_t = s', R_t = r | S_{s-1} = s , A_{t-1} = a\\}$$\n",
    "   \n",
    "What this equation states, is that in a *finite* MDP, there are a limited number of states, actions, and rewards. Because of this, we can discern that the random variables R and S have a probability distribution based only "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "This brings us to building our first algorithm, Q-Learning. Given a state s, and an action a, the Q function returns an estimate of the total reward starting from s and taking a.\n",
    "\n",
    "Let's go over the formula:   \n",
    " \n",
    "$$Q({s_t, a_t}) \\leftarrow Q({s_t, a_t}) + \\alpha[r_{t+1} +\\gamma\\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]$$\n",
    "   \n",
    "$\\alpha$ - the learning rate, typically a small value between 0 and 1, indicates how much we update over values every time we take an action. Typically this value tends to be smaller in order not to overrepresent certain action. However it can also be 1, so that the $Q(s_t, a_t)$ terms cancel out (this is done in DQN).\n",
    "    \n",
    "$\\gamma$ - discount factor, encourages an agent to seek a reward sooner than later, typically set to .9~.99, makes agents receive a smaller reward in the present to give better incentive for future rewards. The effect of the discount factor can be seen when the Bellman equation is expanded, and $\\alpha = 1$.\n",
    "\n",
    "$$Q({s_t, a_t}) = r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\gamma^3 r_ 3 ... $$\n",
    "$$Q({s_t, a_t}) = r_0 + \\gamma(r_1 + \\gamma^2 r_2 + \\gamma^3 r_ 3 ...) = r_0 + \\gamma\\max_a Q(s_{t+1}, a )$$\n",
    "\n",
    "<img src=\"images/q.png\" width=\"400\">\n",
    "\n",
    "Given this formula, you need the apply it using the following steps:\n",
    "1. Set initial value of *Q(s, a)* to all abritary values.   \n",
    "2. Eventually while reaching the limit, make sure to do all actions *a* for all states *s*.\n",
    "3. At each time *t*, change one element.\n",
    "4. You could reduce the $\\alpha$ element over time for optimization purposes.   \n",
    "\n",
    "In order to better visualize this process, we will be going through a sample example based on a program running in OpenAI, something that will be helpful in your exploration of the topic. We will go into more detail on it after this.\n",
    "   \n",
    "### Example\n",
    "First, we create the enviroment. In this case, it is Taxi-v2, an enviroment with the following rules:\n",
    "\n",
    "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. \n",
    "You receive +20 points for a successful dropoff.\n",
    "Lose 1 point for every timestep it takes. \n",
    "10 point penalty for illegal pick-up and drop-off actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in order to start enviroment, we reset it\n",
    "#number returned shows initial RANDOM state between 0-499\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shows total state range\n",
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the image below there are a couple of things to point out:\n",
    "1. Yellow square represents the taxi.\n",
    "2. \"|\" represents a wall.\n",
    "3. Blue letter represents pick-up location.\n",
    "4. Purple Letter is drop-off location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shows the number of actions. down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#override state to 114 for tutorial purposes\n",
    "env.env.s = 114\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns four variables, labeled as state, reward, done, info, in that order \n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterate through the enviroment and attempt to solve it through random choosing considering you only need 20 points to win\n",
    "state = env.reset()\n",
    "counter = 0\n",
    "reward = None\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Form Q table to store values in, 500 * 6, states * actions\n",
    "#alpha assigned randomly\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "G = 0\n",
    "alpha = 0.618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Need to rewrite)\n",
    "Actual Q learning algorithm breakdown:\n",
    "\n",
    "First (#1): The agent starts by choosing an action with the highest Q value for the current state using argmax. Argmax will return the index/action with the highest value for that state. Initially, our Q table will be all zeros. But, after every step, the Q values for state-action pairs will be updated.\n",
    "\n",
    "Second (#2): The agent then takes action and we store the future state as state2 (St+1). This will allow the agent to compare the previous state to the new state.\n",
    "\n",
    "Third (#3): We update the state-action pair (St , At) for Q using the reward, and the max Q value for state2 (St+1). This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values). See Figure 2 for the value iteration update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state]) #1\n",
    "        state2, reward, done, info = env.step(action) #2\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n",
    "        G += reward\n",
    "        state = state2    \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Need to rewrite)\n",
    "Following this update, we update our total reward G and update state (St) to be the previous state2 (St+1) so the loop can begin again and the next action can be decided.\n",
    "\n",
    "After so many episodes, the algorithm will converge and determine the optimal action for every state using the Q table, ensuring the highest possible reward. We now consider the environment problem solved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "<img src=\"images/anakin.png\" width=\"400\">\n",
    "OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. This is the gym open-source library, which gives you access to a standardized set of environments.\n",
    "\n",
    "### Example\n",
    "The following is an example of the game Breakout, and a gif recording of it playing a couple of times. The code is simple and easy to follow so you can get a good idea of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taken from http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "\n",
    "# Run a demo of the environment\n",
    "observation = env.reset()\n",
    "cum_reward = 0\n",
    "frames = []\n",
    "for t in range(1000):\n",
    "    # Render into buffer. \n",
    "    # You will still see the window.\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "#getting error with (close = True) parameter\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays enviroment already record and in GIF form. Technically supposed to not open up on your computer, but have not fixed bug yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"QLEARN CLASS NOT MY CODe\"\"\"\n",
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def load_q(self, new_q):\n",
    "        self.q = new_q\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get(str(state)+ action, 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "        '''\n",
    "        oldv = self.q.get(str(state) + action, None)\n",
    "        if oldv is None:\n",
    "            self.q[str(state)+ action] = reward\n",
    "        else:\n",
    "            self.q[str(state)+ action] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values\n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "\n",
    "        action = self.actions[i]\n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
    "\n",
    "EPSILON=.5\n",
    "ALPHA=.4\n",
    "GAMMA=1\n",
    "\n",
    "env = gym.make(\"BinaryCarRacing-v0\")\n",
    "env.reset()\n",
    "\n",
    "action_list = [\n",
    "    \"steer_left\",\n",
    "    \"steer_right\",\n",
    "    \"gas_on\",\n",
    "    \"gas_off\",\n",
    "    \"brake_on\",\n",
    "    \"brake_off\"\n",
    "]\n",
    "\n",
    "def assign_val(x, index, val):\n",
    "    x[index] = val\n",
    "    return x\n",
    "\n",
    "action_effects = {\n",
    "    \"steer_left\":     lambda x: assign_val(x, 0, -1),\n",
    "    \"steer_straight\": lambda x: assign_val(x, 0, 0),\n",
    "    \"steer_right\":    lambda x: assign_val(x, 0, 1),\n",
    "    \"gas_on\":         lambda x: assign_val(x, 1, 1),\n",
    "    \"gas_off\":        lambda x: assign_val(x, 1, 0),\n",
    "    \"brake_on\":       lambda x: assign_val(x, 2, 1),\n",
    "    \"brake_off\":      lambda x: assign_val(x, 2, 0)\n",
    "}\n",
    "\n",
    "agent = QLearn(\n",
    "    actions=action_list,\n",
    "    epsilon=EPSILON,\n",
    "    alpha=ALPHA,\n",
    "    gamma=GAMMA\n",
    ")\n",
    "\n",
    "max_reading = 0\n",
    "\n",
    "import math\n",
    "def convert_to_discrete(reading, max_reading):\n",
    "    if reading == 0 or reading == -1:\n",
    "        return -1\n",
    "    return math.floor(reading/max_reading * 5)\n",
    "\n",
    "for iteration in range(100):\n",
    "    action = np.array([0, 0, 0])\n",
    "    action_chosen = None\n",
    "    state = None\n",
    "    step_reward = None\n",
    "    done = False\n",
    "    while not done:\n",
    "        new_state, step_reward, done, _ = env.step(action)\n",
    "        for reading in new_state:\n",
    "            if reading > max_reading:\n",
    "                max_reading = reading\n",
    "        #new_state = list(map(lambda x: convert_to_discrete(x, max_reading), new_state))\n",
    "        new_state = [convert_to_discrete(x, max_reading) for x in new_state]\n",
    "        if state:\n",
    "            agent.learn(state, action_chosen, step_reward, new_state)\n",
    "        action_chosen = agent.chooseAction(new_state)\n",
    "        action = action_effects[action_chosen](action)\n",
    "        state = new_state\n",
    "        if iteration % 10 == 0:\n",
    "            env.render()\n",
    "    env.reset()\n",
    "\n",
    "import json\n",
    "with open(\"./agent_q.agent\", \"w+\") as f:\n",
    "    f.write(json.dumps(agent.q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN (Deep Q Network)\n",
    "\n",
    "In 2013, researchers at DeepMind presented one of the first models combining reinforcement learning with a convolutional neural network. Using a neural network, they approximated the Q function, with the state being pixels from the Atari 2600. This model was able to outperform all previous approaches on playing six of the games, and outperforms human experts on three of the games.\n",
    "\n",
    "\n",
    "## Network Training\n",
    "Frames are cropped to 84x84 regions that capture the game playing area and converted to grayscale, then 4 frames are stacked to capture movement at each step. The resulting stack of frames is used as the state at each step.\n",
    "\n",
    "The network is then trained with RMSprop on the mean squared error of $Q(s)$ computed from the network and the actual reward received.  10,000,000 frames were used to train for each game.\n",
    "\n",
    "## Experience Replay\n",
    "Each state, action, reward and new state (known as transitions) are saved in a \"replay memory\", and at each step, a random sample of transitions are taken to train the network with. This is known as experience replay, and has a few benefits, including greater data efficiency (each state transition is used more than once) and more efficient learning (randomly sampled states are less correlated than sequential states). This also avoids oscillation and divergence, because the current state is not entirely dependent on the model's parameters at that time. The replay memory can holds the last 1,000,000 frames.\n",
    "\n",
    "## Target Network\n",
    "The authors of DQN followed up with another technique in which 2 separate Q networks are used, one to train, and one to calculate the target value during training. Every 10,000 steps, the parameters from the trained network are copied over to the target network. This also avoids oscillation and divergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten\n",
    "from keras.models import Model, load_model\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_shape, n_outputs, train=True, model_path=None,\n",
    "                       max_epsilon=1.0, min_epsilon=0.1, gamma=0.99,\n",
    "                       lambda_=0.001, mem_size=5e5,\n",
    "                       batch_size=32, update_target_freq=1000):\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        if train:\n",
    "            # network we are training\n",
    "            self.nn = self._create_nn()\n",
    "\n",
    "            # network we are using to predict targets in Q calculation\n",
    "            self.nn_ = self._create_nn()\n",
    "        else:\n",
    "            self.nn = load_model(model_path)\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "        self.mem_size = mem_size\n",
    "        self.epsilon = max_epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.batch_size = batch_size\n",
    "        self.update_target_freq = update_target_freq\n",
    "\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "\n",
    "    def _create_nn(self):\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        net = Conv2D(16, 8, strides=4, activation='relu')(inputs)\n",
    "        net = Conv2D(32, 4, strides=2, activation='relu')(net)\n",
    "        net = Flatten()(net)\n",
    "        net = Dense(256, activation='relu')(net)\n",
    "        outputs = Dense(self.n_outputs, activation='linear')(net)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile('rmsprop', 'mse')\n",
    "        return model\n",
    "\n",
    "    def _get_batch(self):\n",
    "        n = min(len(self.memory), self.batch_size)\n",
    "        return random.sample(self.memory, n)\n",
    "\n",
    "    def _decrease_epsilon(self):\n",
    "        self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon)\\\n",
    "                       * math.exp(-self.lambda_ * self.steps)\n",
    "\n",
    "    def _update_target_nn(self):\n",
    "        self.nn_.set_weights(self.nn.get_weights())\n",
    "\n",
    "    def action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_outputs - 1)\n",
    "        else:\n",
    "            if state.ndim == 3:\n",
    "                state = np.expand_dims(state, axis=0)\n",
    "            return np.argmax(self.nn.predict(state))\n",
    "\n",
    "    def observe(self, sample):\n",
    "        \"\"\"\n",
    "        sample should consist of state, action, reward, state_\n",
    "        \"\"\"\n",
    "        if len(self.memory) > self.mem_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(sample)\n",
    "\n",
    "        self.steps += 1\n",
    "        self._decrease_epsilon()\n",
    "\n",
    "        if self.steps % self.update_target_freq == 0:\n",
    "            self._update_target_nn()\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        batch = self._get_batch()\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for s, a, r, s_ in batch:\n",
    "            X.append(s)\n",
    "            y.append(self.Q(s).ravel())\n",
    "\n",
    "            if s_ is None:\n",
    "                y[-1][a] = r\n",
    "            else:\n",
    "                y[-1][a] = r + self.gamma*np.amax(self.Q(s_, target=True))\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.nn.fit(X, y, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "    def Q(self, state, target=False):\n",
    "        if state.ndim == 3:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "\n",
    "        if target:\n",
    "            return self.nn_.predict(state)\n",
    "        else:\n",
    "            return self.nn.predict(state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "RENDER = True\n",
    "TRAIN = False\n",
    "MODEL_PATH = 'breakout.h5'\n",
    "\n",
    "def to_gray(rgb):\n",
    "    gray = np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "    return gray\n",
    "\n",
    "\n",
    "def process(frame):\n",
    "    frame = imresize(frame, (110, 84, 3))\n",
    "    frame = to_gray(frame)\n",
    "    frame = frame[20:104]\n",
    "    return frame\n",
    "\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "\"\"\"\n",
    "Actions in Breakout:\n",
    "    0 - Noop\n",
    "    1 - Fire (start game)\n",
    "    2 - Right\n",
    "    3 - Left\n",
    "\n",
    "State: 210x160x3 Image\n",
    "\"\"\"\n",
    "dqn_agent = DQNAgent((84, 84, 4), 4, train=TRAIN, model_path=MODEL_PATH)\n",
    "\n",
    "frames = []\n",
    "for ep_i in range(NUM_EPISODES):\n",
    "    frame = env.reset()\n",
    "    frame = process(frame)\n",
    "\n",
    "    # Fill frame history with initial state\n",
    "    frames += [frame, frame, frame, frame]\n",
    "    state = np.stack(frames, axis=2)\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = dqn_agent.action(state)\n",
    "        frame, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "            # If playing, sleep so humans can watch\n",
    "            if not TRAIN:\n",
    "                time.sleep(1/20)\n",
    "\n",
    "        frame = process(frame)\n",
    "        frames.pop()\n",
    "        frames.append(frame)\n",
    "        state_ = np.stack(frames, axis=2)\n",
    "\n",
    "        if TRAIN:\n",
    "            dqn_agent.observe((state, action, reward, state_))\n",
    "            dqn_agent.replay()\n",
    "\n",
    "        state = state_\n",
    "\n",
    "    print('Episode %d finished with reward %d' % (ep_i, total_reward))\n",
    "\n",
    "\n",
    "if TRAIN:\n",
    "    dqn_agent.nn.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "DQN is a value based method, where Q approximates a value, such as the reward, and the policy is implicit,\n",
    "with actions selected using a greedy policy.\n",
    "\n",
    "Another type of algorithms exist known as policy gradient methods, which given a state, return the approximate \"best\" action, so the policy is explicit. The policy is parameterized, and we can do gradient descent into the direction that improves it. This can sometimes be easier to approximate than value functions, is needed for continuous action spaces and environments and can learn stochastic policies.\n",
    "\n",
    "This alsos transfers to neural networks more naturally, as policy based reinforcment learning is an optimization problem, as opposed to value based reinforcment learning which is a function approximation problem.\n",
    "\n",
    "## Policy Objective Functions\n",
    "\n",
    "The quality of the policy can be measured with either the start value, average value, or average reward per time-step. This measure gives us the objective function, $J(\\theta)$ that we want to optimize.\n",
    "\n",
    "## Policy Gradient Theorem\n",
    "\n",
    "The policy gradient theorem defines the gradient of the policy objective function:\n",
    "\n",
    "For a differentiable policy $\\pi_\\theta(s, a)$, and a policy objective function $J(\\theta)$, the policy gradient\n",
    "is\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbf{E}_{\\pi_\\theta} [ \\nabla_\\theta \\log \\pi_\\theta(s, a) Q^{\\pi_\\theta}(s, a) ]$$\n",
    "\n",
    "We can use this gradient to move the policy to the direction of more reward.\n",
    "\n",
    "## Monte-Carlo Policy Gradient (REINFORCE)\n",
    "\n",
    "In the above policy gradient, we can use the return $v_t$ (discoutned sum of rewards) as an unbiased sample\n",
    "of $Q^{\\pi_\\theta}(s, a)$. This is known as the REINFORCE algorithm.\n",
    "\n",
    "\n",
    "## Actor Critic Methods\n",
    "We can also use an approximation of $Q$ to act as the \"critic\" to critique the policy or \"actor\". $Q$ is updated throughout the learning process.\n",
    "\n",
    "## Stochastic vs Deterministic Policy Gradients\n",
    "A deterministic policy will return the same action with the highest expected Q value, whereas a stocashtic policy models a distribution over all actions, and selects the next action according to this distribution.\n",
    "\n",
    "## Deterministic Policy Gradients\n",
    "A deterministic policy gradient is simply the expected gradient of the action-value function. Due to its nature, it can be calculated more efficiently than its stochastic counterpart, especially in a high-dimensional action space.\n",
    "\n",
    "## Deep Deterministic Policy Gradients\n",
    "Example?: http://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html\n",
    "\n",
    "## Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Image #1: http://slideplayer.com/slide/2342696/\n",
    "\n",
    "Image #2: Title Book\n",
    "\n",
    "Image #3: https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/\n",
    "\n",
    "Youtube: https://youtu.be/ggqnxyjaKe4\n",
    "\n",
    "Youtube: https://youtu.be/3T5eCou2erg\n",
    "\n",
    "Code/Content: https://github.com/dennybritz/reinforcement-learning\n",
    "\n",
    "Code/Content #2:https://github.com/vmayoral/basic_reinforcement_learning\n",
    "\n",
    "Code/Content #3:https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n",
    "\n",
    "DQN: https://docs.google.com/viewer?url=https%3A%2F%2Fwww.cs.toronto.edu%2F~vmnih%2Fdocs%2Fdqn.pdf\n",
    "\n",
    "DQN (Nature): https://docs.google.com/viewer?url=https%3A%2F%2Fstorage.googleapis.com%2Fdeepmind-media%2Fdqn%2FDQNNaturePaper.pdf\n",
    "\n",
    "https://jaromiru.com/2016/09/27/lets-make-a-dqn-theory/\n",
    "\n",
    "Policy Gradient Info: https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient\n",
    "Policy Gradient Slides: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf\n",
    "Simple Policy Gradient with Neural Network Example: http://karpathy.github.io/2016/05/31/rl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
